{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Introduction to Sentence Segmentation\n",
    "\n",
    "Sentence segmentation, means, to split a given paragraph of text into sentences, by identifying the sentence boundaries. In many cases, a full stop is all that is required to identify the end of a sentence, but the task is not all that simple.\n",
    "\n",
    "This is an open ended challenge to which there are no perfect solutions. Try to break up given paragraphs into text into individual sentences. Even if you don't manage to segment the text perfectly, the more sentences you identify and display correctly, the more you will score.\n",
    "\n",
    "Abbreviations: Dr. W. Watson is amazing. In this case, the first and second \".\" occurs after Dr (Doctor) and W (initial in the person's name) and should not be confused as the end of the sentence.\n",
    "\n",
    "Sentences enclosed in quotes: \"What good are they? They're led about just for show!\" remarked another. All of this, should be identified as just one sentence.\n",
    "\n",
    "Questions and exclamations: Who is it? -This is a question. This should be identified as a sentence. I am tired!: Something which has been exclaimed. This should also be identified as a sentence.\n",
    "\n",
    "INPUT FORMAT\n",
    "\n",
    "You will be given a chunk of text, containing several sentences, questions, statements and exclamations- all in 1 line.\n",
    "\n",
    "Constraints\n",
    "\n",
    "Number of characters in every input does not exceed 10000.\n",
    "Number of words in every input does not exceed 1000. There will be more than 1 sentence in each input and this number does not exceed 30.\n",
    "There will be more than 2 characters in every expected sentence and this number does not exceed 10000. There will be more than 2 characters in every test file and this number does not exceed 10000.\n",
    "\n",
    "OUTPUT FORMAT\n",
    "\n",
    "You will split the chunk of text into sentences, and display one sentence per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    # Define regex patterns for identifying sentence boundaries\n",
    "    sentence_endings = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\:)\\s'\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    # Return the list of sentences\n",
    "    return sentences\n",
    "\n",
    "# Read input text from stdin\n",
    "input_text = input().strip()\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = split_sentences(input_text)\n",
    "\n",
    "# Print each sentence on a separate line\n",
    "for sentence in sentences:\n",
    "    print(sentence.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity of a bigram model is 170. Compute its cross-entropy corrected to 2 decimal places.\n",
    "\n",
    "You may either submit the final answer in the plain-text mode, or you may submit a program in the language of your choice to compute the required value.\n",
    "\n",
    "Your answer should look like this:\n",
    "\n",
    "5.50  \n",
    "Do not use any extra leading or trailing spaces or newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.41\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
    "import math\n",
    "\n",
    "perplexity = 170\n",
    "cross_entropy = math.log2(perplexity)\n",
    "\n",
    "print(\"{:.2f}\".format(cross_entropy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deterministic Url and HashTag Segmentation\n",
    "\n",
    "This problem will introduce you to the segmentation of Domain Names and Social Media HashTags, into English Language words. To give you a quick idea of what segmentation means, here are a few examples of Domain names and Hash Tags which have been segmented.\n",
    "\n",
    "Domain Name Examples:-\n",
    "\n",
    "www.checkdomain.com => [check domain]\n",
    "\n",
    "www.bigrock.com => [big rock]\n",
    "\n",
    "www.namecheap.com => [name cheap]\n",
    "\n",
    "www.appledomains.in => [apple domains]\n",
    "\n",
    "Twitter Hash Tag Examples:\n",
    "\n",
    "#honestyhour => [honesty hour]\n",
    "\n",
    "#beinghuman => [being human]\n",
    "\n",
    "#followback => [follow back]\n",
    "\n",
    "#socialmedia => [social media]\n",
    "\n",
    "#30secondstoearth => [30 seconds to earth]\n",
    "\n",
    "The segmentation should be based on the list of 5000 most common words from here Apart from the words in this list, you should also pick up numbers (both integer and decimal) like 100, 200.10 etc.\n",
    "\n",
    "At this stage, we are going to use a very simple algorithm for the process. In case the input is a domain name, ignore the www. and/or the extensions (.com,.edu,.org,.in, etc.) In case the input is a hashtag, ignore the first # symbol. Split the input string, into a sequence of tokens. A token can either be:\n",
    "\n",
    "A word in from the provided lexicon/dictionary.\n",
    "An integer or decimal number.\n",
    "There might be cases where it might be possible to parse (or split) an input string into tokens in multiple possible ways.\n",
    "\n",
    "currentratesoughttogodown \n",
    "This can be split into:\n",
    "\n",
    "current rate sought to go down\n",
    "\n",
    "current rates ought to go down.\n",
    "\n",
    "thisisinsane\n",
    "\n",
    "This can be split into:\n",
    "\n",
    "this is in sane\n",
    "this is insane\n",
    "Write your splitter in such a way, that as you tokenise a string from left to right; in case there are multiple possible ways to split the string,\n",
    "\n",
    "select the longest possible string from the left side, such that the remaining string can be split into valid tokens. So, for the two cases above, the appropriate ways to split the strings are:\n",
    "\n",
    "current rates ought to go down\n",
    "this is insane\n",
    "In case there is no valid way to split the string into a valid sequence of tokens, output the original string itself, after scrubbing out the # for hashtags, the 'www' and extensions for domain names.\n",
    "\n",
    "Input Format\n",
    "\n",
    "First line will contain the number of test cases N\n",
    "\n",
    "This will be followed by N inputs on separate lines, which will contain twitter hash-tags and domain names, which you need to segment\n",
    "\n",
    "There will be a file named \"words.txt\" in the run directory of your program that contains all of the words each seperated by a new line. It is the same file that is linked earlier in the problem statement.\n",
    "\n",
    "Output Format\n",
    "\n",
    "(Everything should be in lower case)\n",
    "\n",
    "Segmentation for Input 1  \n",
    "Segmentation for Input 2\n",
    "             .\n",
    "             .\n",
    "Segmentation for Input N\n",
    "Sample Input\n",
    "\n",
    "3\n",
    "#isittime  \n",
    "www.whatismyname.com  \n",
    "#letusgo  \n",
    "Sample Output\n",
    "\n",
    "is it time  \n",
    "what is my name  \n",
    "let us go\n",
    "The sample input is just to get an idea of what to do. Your program is not expected to be able to run it. You can also read the corpus of words by making your program read the file \"words.txt\" from its current directory.\n",
    "\n",
    "Please note, that the \"words.txt\" file, is a list of several common words, but it will not necessarily produce the ideal natural language segmentation for each of the examples, samples or tests. That is the expected behavior: we are only trying to gauge how well you can segment these hashtags and domain names, with this limited list of words.\n",
    "\n",
    "Scoring\n",
    "\n",
    "All test cases have equal weightage.\n",
    "\n",
    "Score = M * (C)/N Where M is the Maximum Score for the test case.\n",
    "C = Number of correct answers in your output.\n",
    "N = Total number of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
    "def load_words(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        words = set(word.strip().lower() for word in file)\n",
    "    return words\n",
    "\n",
    "def segment_string(input_string, dictionary):\n",
    "    input_string = input_string.lower()\n",
    "    if input_string.startswith('#'):\n",
    "        input_string = input_string[1:]  # Remove leading '#' for hashtags\n",
    "    elif input_string.startswith('www.'):\n",
    "        input_string = input_string[4:]  # Remove leading 'www.' for domain names\n",
    "    input_string = input_string.split('.')[0]  # Remove domain extensions\n",
    "    \n",
    "    segmented = []\n",
    "    while input_string:\n",
    "        found = False\n",
    "        for i in range(len(input_string), 0, -1):\n",
    "            token = input_string[:i]\n",
    "            if token in dictionary or token.isdigit():\n",
    "                segmented.append(token)\n",
    "                input_string = input_string[i:].strip()\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            segmented.append(input_string)\n",
    "            break\n",
    "    \n",
    "    return ' '.join(segmented)\n",
    "\n",
    "# Load dictionary\n",
    "dictionary = load_words(\"words.txt\")\n",
    "\n",
    "# Read number of test cases\n",
    "N = int(input())\n",
    "\n",
    "# Process each test case\n",
    "for _ in range(N):\n",
    "    input_string = input().strip()\n",
    "    segmented_string = segment_string(input_string, dictionary)\n",
    "    print(segmented_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the POS-tagged version of a sentence. The tags that are used are the most commonly used standard from the Pen Tree Tagset.\n",
    "\n",
    "Your task is to fill in the missing tags which have been replaced by question marks. The missing tags will be restricted to the set of tags which you already see in the POS tagged version of this sentence.\n",
    "If there are two question marks (??), it indicates a 2-letter tag (CC, JJ, NN etc.).\n",
    "If there are three question marks (???), it indicates a 3-letter tag (NNP, PPS, VBP).\n",
    "\n",
    "The/DT planet/NN Jupiter/NNP and/CC its/PPS moons/NNS are/VBP in/IN effect/NN a/DT minisolar/JJ system/?? ,/, and/CC Jupiter/NNP itself/PRP is/VBZ often/RB called/VBN a/DT star/?? that/IN never/RB caught/??? fire/NN ./.\n",
    "In the plain-text box below, submit the sentence, as is, with only the question marks filled in appropriately and nothing else changed.\n",
    "\n",
    "For example, your answer may look like this (This is not the correct answer):\n",
    "\n",
    "The/DT planet/NN Jupiter/NNP and/CC its/PPS moons/NNS are/VBP in/IN effect/NN a/DT minisolar/JJ system/CC ,/, and/CC Jupiter/NNP itself/PRP is/VBZ often/RB called/VBN a/DT star/CC that/IN never/RB caught/NNP fire/NN ./."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The/DT planet/NN Jupiter/NNP and/CC its/PPS moons/NNS are/VBP in/IN effect/NN a/DT minisolar/JJ system/?? ,/, and/CC Jupiter/NNP itself/PRP is/VBZ often/RB called/VBN a/DT star/?? that/IN never/RB caught/??? fire/NN ./.\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_tags(sentence):\n",
    "    tokens = sentence.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if '?' in token:\n",
    "            missing_length = token.count('?')\n",
    "            if missing_length == 2:\n",
    "                for j in range(i-1, -1, -1):\n",
    "                    if len(tokens[j]) == 2 and tokens[j].isalpha():\n",
    "                        tokens[i] = tokens[i].replace('??', tokens[j])\n",
    "                        break\n",
    "            elif missing_length == 3:\n",
    "                for j in range(i-1, -1, -1):\n",
    "                    if len(tokens[j]) == 3 and tokens[j].isalpha():\n",
    "                        tokens[i] = tokens[i].replace('???', tokens[j])\n",
    "                        break\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Provided test case\n",
    "test_case = \"The/DT planet/NN Jupiter/NNP and/CC its/PPS moons/NNS are/VBP in/IN effect/NN a/DT minisolar/JJ system/?? ,/, and/CC Jupiter/NNP itself/PRP is/VBZ often/RB called/VBN a/DT star/?? that/IN never/RB caught/??? fire/NN ./.\"\n",
    "\n",
    "# Fill in missing POS tags\n",
    "result = fill_missing_tags(test_case)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
